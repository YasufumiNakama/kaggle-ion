{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Directory Settings\n",
    "# =====================================================================================\n",
    "ROOT = '../input/liverpool-ion-switching/'\n",
    "CLEAN_ROOT = '../input/data-without-drift/'\n",
    "OUTPUT_DIR = '.'\n",
    "MODEL_DIR1 = '../input/ion-stack1/'\n",
    "MODEL_DIR2 = '../input/ion-stack3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    learning_rate=3.0e-3\n",
    "    batch_size=32\n",
    "    num_workers=6\n",
    "    print_freq=1000\n",
    "    test_freq=1\n",
    "    start_epoch=0\n",
    "    num_train_epochs=150\n",
    "    warmup_steps=30\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    weight_decay=0.01\n",
    "    dropout=0.3\n",
    "    emb_size=100\n",
    "    hidden_size=164\n",
    "    nlayers=2\n",
    "    nheads=10\n",
    "    seq_len=5000\n",
    "    total_cate_size=40\n",
    "    seed=1225\n",
    "    encoder='Wavenet'\n",
    "    optimizer='Adam' #@param ['AdamW', 'Adam']\n",
    "    target_size=11\n",
    "    n_fold=4\n",
    "    fold=[1, 3] #[0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Library\n",
    "# =====================================================================================\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Utils\n",
    "# =====================================================================================\n",
    "def get_logger(filename=OUTPUT_DIR+'log'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    logger.info(f'[{name}] start')\n",
    "    yield\n",
    "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def load_df(path, debug=False):\n",
    "    # load df .csv or .pkl\n",
    "    if path.split('.')[-1]=='csv':\n",
    "        df = pd.read_csv(path)\n",
    "        if debug:\n",
    "            df = pd.read_csv(path, nrows=1000)\n",
    "    elif path.split('.')[-1]=='pkl':\n",
    "        df = pd.read_pickle(path)\n",
    "    print(f\"{path} shape / {df.shape} \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# General Settings\n",
    "# =====================================================================================\n",
    "df_path_dict = {'train': CLEAN_ROOT+'train_clean.csv',\n",
    "                'test': CLEAN_ROOT+'test_clean.csv',\n",
    "                'sample_submission': ROOT+'sample_submission.csv'}\n",
    "ID = 'time'\n",
    "TARGET = 'open_channels'\n",
    "SEED = 42\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/data-without-drift/train_clean.csv shape / (5000000, 3) \n",
      "../input/data-without-drift/test_clean.csv shape / (2000000, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] done in 8 s\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# Data Loading\n",
    "# =====================================================================================\n",
    "with timer('Data Loading'):\n",
    "    X_train = load_df(path=df_path_dict['train'])\n",
    "    X_test = load_df(path=df_path_dict['test'])\n",
    "    submission = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n",
    "    oof_lgb = pd.read_csv('../input/ion-oof/oof_lightgbm.csv').rename(columns={TARGET:'signal_lgb_oof'})\n",
    "    pred_lgb = pd.read_csv('../input/ion-oof/predictions_lightgbm.csv').rename(columns={TARGET:'signal_lgb_oof'})\n",
    "    oof_cat = pd.read_csv('../input/ion-oof/oof_catboost.csv').rename(columns={TARGET:'signal_cat_oof'})\n",
    "    pred_cat = pd.read_csv('../input/ion-oof/predictions_catboost.csv').rename(columns={TARGET:'signal_cat_oof'})\n",
    "\n",
    "    X_train = pd.concat([X_train, oof_lgb['signal_lgb_oof']], axis=1)\n",
    "    X_test = pd.concat([X_test, pred_lgb['signal_lgb_oof']], axis=1)\n",
    "    X_train = pd.concat([X_train, oof_cat['signal_cat_oof']], axis=1)\n",
    "    X_test = pd.concat([X_test, pred_cat['signal_cat_oof']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Preprocess\n",
    "# =====================================================================================\n",
    "X_train['batch'] = X_train.index // 500000\n",
    "X_train['batch'] = X_train['batch'].astype(int)\n",
    "X_test['batch'] = X_test.index // 500000\n",
    "X_test['batch'] = X_test['batch'].astype(int)\n",
    "\n",
    "\n",
    "def signal2cate(X_train, X_test=None, NUM_BINS=1000):\n",
    "    signal_bins = np.linspace(X_train['signal'].min(), X_train['signal'].max(), NUM_BINS + 1)\n",
    "    train_signal_dig = np.digitize(X_train['signal'], bins=signal_bins) - 1\n",
    "    train_signal_dig = np.minimum(train_signal_dig, len(signal_bins) - 2)\n",
    "    X_train['signal_cate'] = train_signal_dig\n",
    "    if X_test is not None:\n",
    "        test_signal_dig = np.digitize(X_test['signal'], bins=signal_bins) - 1\n",
    "        test_signal_dig = np.minimum(test_signal_dig, len(signal_bins) - 2)\n",
    "        X_test['signal_cate'] = test_signal_dig\n",
    "        return  X_train, X_test\n",
    "    return X_train\n",
    "\n",
    "X_train, X_test = signal2cate(X_train, X_test, NUM_BINS=CFG.total_cate_size)\n",
    "\n",
    "\n",
    "def add_num_features(X_train, X_test=None):\n",
    "    max_signal = X_train['signal'].max()\n",
    "    min_signal = X_train['signal'].min()\n",
    "    X_train['signal_diff_max'] = max_signal - X_train['signal']\n",
    "    X_train['signal_diff_min'] = min_signal - X_train['signal']\n",
    "    if X_test is not None:\n",
    "        X_test['signal_diff_max'] = max_signal - X_test['signal']\n",
    "        X_test['signal_diff_min'] = min_signal - X_test['signal']\n",
    "        return  X_train, X_test\n",
    "    return X_train\n",
    "\n",
    "X_train, X_test = add_num_features(X_train, X_test)\n",
    "\n",
    "\n",
    "def calc_gradients(df):\n",
    "\n",
    "    df['signal_gradient'] = np.gradient(df['signal'].values)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df):\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "\n",
    "    for i in range(int(len(df)/500000)):\n",
    "        tmp = df.loc[i * 500000: 500000*(i + 1) - 1].reset_index(drop=True)\n",
    "        tmp = calc_gradients(tmp)\n",
    "        output = pd.concat([output, tmp])\n",
    "\n",
    "    return output.reset_index(drop=True)\n",
    "\n",
    "X_train = preprocess_df(X_train)\n",
    "X_test = preprocess_df(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Dataset\n",
    "# =====================================================================================\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, sample_indices, cfg):\n",
    "        self.df = df.copy()\n",
    "        self.target = df[TARGET].values\n",
    "        self.sample_indices = sample_indices\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = self.cfg.seq_len\n",
    "        self.cont_cols = self.cfg.cont_cols\n",
    "        self.cate_cols = self.cfg.cate_cols\n",
    "        self.cont_df = self.df[self.cont_cols]\n",
    "        self.cate_df = self.df[self.cate_cols]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sample_indices[idx]\n",
    "        seq_len = min(self.seq_len, len(indices))\n",
    "\n",
    "        tmp_cont_x = torch.FloatTensor(self.cont_df.iloc[indices].values)\n",
    "        cont_x = torch.FloatTensor(self.seq_len, len(self.cont_cols)).zero_()\n",
    "        cont_x[-seq_len:] = tmp_cont_x[-seq_len:]\n",
    "\n",
    "        tmp_cate_x = torch.LongTensor(self.cate_df.iloc[indices].values)\n",
    "        cate_x = torch.LongTensor(self.seq_len, len(self.cate_cols)).zero_()\n",
    "        cate_x[-seq_len:] = tmp_cate_x[-seq_len:]\n",
    "\n",
    "        target = np.zeros(self.seq_len) - 1\n",
    "        target[-seq_len:] = self.target[indices]\n",
    "\n",
    "        return cate_x, cont_x, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, sample_indices, cfg):\n",
    "        self.df = df.copy()\n",
    "        self.sample_indices = sample_indices\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = self.cfg.seq_len\n",
    "        self.cont_cols = self.cfg.cont_cols\n",
    "        self.cate_cols = self.cfg.cate_cols\n",
    "        self.cont_df = self.df[self.cont_cols]\n",
    "        self.cate_df = self.df[self.cate_cols]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sample_indices[idx]\n",
    "        seq_len = min(self.seq_len, len(indices))\n",
    "\n",
    "        tmp_cont_x = torch.FloatTensor(self.cont_df.iloc[indices].values)\n",
    "        cont_x = torch.FloatTensor(self.seq_len, len(self.cont_cols)).zero_()\n",
    "        cont_x[-seq_len:] = tmp_cont_x[-seq_len:]\n",
    "\n",
    "        tmp_cate_x = torch.LongTensor(self.cate_df.iloc[indices].values)\n",
    "        cate_x = torch.LongTensor(self.seq_len, len(self.cate_cols)).zero_()\n",
    "        cate_x[-seq_len:] = tmp_cate_x[-seq_len:]\n",
    "\n",
    "        return cate_x, cont_x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Wavenet Model\n",
    "# =====================================================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class wave_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=int((kernel_size + (kernel_size-1)*(dilation-1))/2), dilation=dilation)\n",
    "        self.conv3 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=int((kernel_size + (kernel_size-1)*(dilation-1))/2), dilation=dilation)\n",
    "        self.conv4 = nn.Conv1d(out_ch, out_ch, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_x = x\n",
    "        tanh = self.tanh(self.conv2(x))\n",
    "        sig = self.sigmoid(self.conv3(x))\n",
    "        x = tanh.mul(sig)\n",
    "        x = self.conv4(x)\n",
    "        x = res_x + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wavenet(nn.Module):\n",
    "    def __init__(self, cfg, basic_block=wave_block):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.basic_block = basic_block\n",
    "        cont_col_size = len(cfg.cont_cols)\n",
    "        cate_col_size = len(cfg.cate_cols)\n",
    "        self.cate_emb = nn.Embedding(cfg.total_cate_size, cfg.emb_size, padding_idx=0)\n",
    "        #self.cate_proj = nn.Sequential(\n",
    "        #    nn.Linear(cfg.emb_size*cate_col_size, cfg.hidden_size//2),\n",
    "        #    nn.LayerNorm(cfg.hidden_size//2),\n",
    "        #)\n",
    "        self.layer1 = self._make_layers(cont_col_size, cfg.hidden_size//16, 3, 12)\n",
    "        self.layer2 = self._make_layers(cfg.hidden_size//16, cfg.hidden_size//8, 3, 8)\n",
    "        self.layer3 = self._make_layers(cfg.hidden_size//8, cfg.hidden_size//4, 3, 4)\n",
    "        self.layer4 = self._make_layers(cfg.hidden_size//4, cfg.hidden_size//2, 3, 1)\n",
    "        self.gru = nn.GRU(input_size=cfg.emb_size*cate_col_size+cont_col_size, hidden_size=cfg.hidden_size//4, num_layers=cfg.nlayers,\n",
    "                          bidirectional=True, batch_first=True, dropout=cfg.dropout)\n",
    "        def get_reg():\n",
    "            return nn.Sequential(\n",
    "            nn.Linear(cfg.hidden_size, cfg.hidden_size),\n",
    "            nn.LayerNorm(cfg.hidden_size),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.hidden_size, cfg.hidden_size),\n",
    "            nn.LayerNorm(cfg.hidden_size),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.hidden_size, cfg.target_size),\n",
    "        )\n",
    "        self.fc = get_reg()\n",
    "\n",
    "    def _make_layers(self, in_ch, out_ch, kernel_size, n):\n",
    "        dilation_rates = [2 ** i for i in range(n)]\n",
    "        layers = [nn.Conv1d(in_ch, out_ch, 1)]\n",
    "        for dilation in dilation_rates:\n",
    "            layers.append(self.basic_block(out_ch, out_ch, kernel_size, dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, cate_x, cont_x):\n",
    "        batch_size = cate_x.size(0)\n",
    "        # RNN\n",
    "        cate_emb = self.cate_emb(cate_x).view(batch_size, self.cfg.seq_len, -1)\n",
    "        seq_emb = torch.cat((cate_emb, cont_x), 2)\n",
    "        h_gru, _ = self.gru(seq_emb)\n",
    "        # CNN\n",
    "        cont_x = cont_x.permute(0, 2, 1)\n",
    "        cont_x = self.layer1(cont_x)\n",
    "        cont_x = self.layer2(cont_x)\n",
    "        cont_x = self.layer3(cont_x)\n",
    "        cont_x = self.layer4(cont_x)\n",
    "        # CNN & RNN\n",
    "        x = torch.cat((cont_x, h_gru.permute(0, 2, 1)), 1).permute(0, 2, 1)\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Get Sample function\n",
    "# =====================================================================================\n",
    "def tta_group(X_train):\n",
    "    tmp = X_train[CFG.seq_len//2:len(X_train)-CFG.seq_len//2].reset_index(drop=True)\n",
    "    tmp['tta_group'] = tmp.index // CFG.seq_len\n",
    "    tmp['tta_group'] = tmp['tta_group'] + tmp['group'].nunique()\n",
    "    X_train = pd.concat([X_train[:CFG.seq_len//2], \n",
    "                         tmp, \n",
    "                         X_train[len(X_train)-CFG.seq_len//2:]]).reset_index(drop=True).fillna(-1)\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def get_sample_indices(df):\n",
    "    sample_indices = []\n",
    "    group_indices = []\n",
    "    df_groups = df.groupby('group').groups\n",
    "    tta_df_groups = df[df['tta_group']>=0].groupby('tta_group').groups\n",
    "    for group_idx, indices in enumerate(df_groups.values()):\n",
    "        sample_indices.append(indices.values)\n",
    "        group_indices.append(group_idx)\n",
    "    for group_idx, indices in enumerate(tta_df_groups.values()):\n",
    "        sample_indices.append(indices.values)\n",
    "        group_indices.append(group_idx+len(df_groups))\n",
    "    return np.array(sample_indices), group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# validate function\n",
    "# =====================================================================================\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def validate(valid_loader, model, device):\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "    \n",
    "    tk0 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "\n",
    "    for step, (cate_x, cont_x, y) in tk0:\n",
    "\n",
    "        cate_x, cont_x, y = cate_x.to(device), cont_x.to(device), y.to(device)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_x, cont_x)\n",
    "\n",
    "            # record loss\n",
    "            pred_ = pred.view(-1, pred.shape[-1])\n",
    "            y_ = y.view(-1)\n",
    "\n",
    "        # record accuracy\n",
    "        val_true = torch.cat([val_true, y_.long()], 0)\n",
    "        val_preds = torch.cat([val_preds, pred_], 0)\n",
    "\n",
    "    # scoring\n",
    "    predictions = val_preds.cpu().detach().numpy().argmax(1)\n",
    "    groundtruth = val_true.cpu().detach().numpy()\n",
    "    score = f1_score(predictions, groundtruth, labels=list(range(11)), average='macro')\n",
    "    logger.info(f'score: {score}')\n",
    "\n",
    "    return predictions, groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Inference function\n",
    "# =====================================================================================\n",
    "def inference(test_loader, model, device):\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    probs = torch.Tensor([]).to(device)\n",
    "    \n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    \n",
    "    for step, (cate_x, cont_x) in tk0:      \n",
    "        \n",
    "        cate_x, cont_x = cate_x.to(device), cont_x.to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            pred = model(cate_x, cont_x)\n",
    "            pred_ = pred.view(-1, pred.shape[-1])\n",
    "        \n",
    "        probs = torch.cat([probs, pred_], 0)\n",
    "\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# CV\n",
    "# =====================================================================================\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def check_CV(fold, trn_idx, val_idx, model_path):\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # Settings\n",
    "    # =====================================================================================\n",
    "    cate_cols = [c for c in X_train.columns if c.find('cate')>=0]\n",
    "    if fold==1:\n",
    "        cont_cols = ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
    "    else:\n",
    "        cont_cols = ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
    "    logger.info(f'cont_cols: {cont_cols}')\n",
    "    logger.info(f'cate_cols: {cate_cols}')\n",
    "    CFG.cont_cols = cont_cols\n",
    "    CFG.cate_cols = cate_cols\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f'device: {device}')\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare loader\n",
    "    # =====================================================================================\n",
    "    #train_samples = sample_indices[trn_idx]\n",
    "    valid_samples = sample_indices[val_idx]\n",
    "\n",
    "    #train_db = TrainDataset(X_train, train_samples, CFG)\n",
    "    valid_db = TrainDataset(X_train, valid_samples, CFG)\n",
    "\n",
    "    #train_loader = DataLoader(train_db, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_db, batch_size=CFG.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare model\n",
    "    # =====================================================================================\n",
    "    model = Wavenet(CFG)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    logger.info(checkpoint['log'].sort_values(['VALID_SCORE']).tail(1))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # CV\n",
    "    # =====================================================================================\n",
    "    predictions, groundtruth = validate(valid_loader, model, device)\n",
    "    \n",
    "    return predictions, groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train['group'] = X_train.index // CFG.seq_len\\nX_train = tta_group(X_train)\\nX_train['tta_group'] = X_train['tta_group'].astype(int)\\nX_train['split_group'] = X_train.index // (CFG.seq_len*2)\\nsample_indices, group_indices = get_sample_indices(X_train)\\nprint(len(group_indices))\\nprint(len(set(group_indices)))\\ngroup_map = dict(X_train[['group', 'split_group']].values.tolist())\\ntta_group_map = dict(X_train[['tta_group', 'split_group']].values.tolist())\\ngroup_map.update(tta_group_map)\\ngroup_indices = [group_map[i] for i in group_indices]\\nprint(len(group_indices))\\nprint(len(set(group_indices)))\\nfolds = pd.read_csv('../input/ion-folds/folds.csv')\\nfolds_map = dict(folds[['split_group', 'fold']].values.tolist())\\ngroup_indices = [folds_map[i] for i in group_indices]\\nskf = GroupKFold(n_splits=CFG.n_fold)\\nsplits = [x for x in skf.split(sample_indices, None, group_indices)]\\npredictions, groundtruth = [], []\\nfor fold, (trn_idx, val_idx) in enumerate(splits):\\n    if fold in CFG.fold:\\n        with timer(f'##### Running Fold: {fold} #####'):\\n            if fold==1:\\n                model_path = MODEL_DIR1 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'                                + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\\n            else:\\n                model_path = MODEL_DIR2 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'                                + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\\n            _predictions, _groundtruth = check_CV(fold, trn_idx, val_idx, model_path)\\n            predictions.append(_predictions)\\n            groundtruth.append(_groundtruth)\\npredictions = np.concatenate(predictions)\\ngroundtruth = np.concatenate(groundtruth)\\nscore = f1_score(predictions, groundtruth, labels=list(range(11)), average='macro')\\nlogger.info(f'##### CV Score: {score} #####')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train['group'] = X_train.index // CFG.seq_len\n",
    "X_train = tta_group(X_train)\n",
    "X_train['tta_group'] = X_train['tta_group'].astype(int)\n",
    "X_train['split_group'] = X_train.index // (CFG.seq_len*2)\n",
    "sample_indices, group_indices = get_sample_indices(X_train)\n",
    "print(len(group_indices))\n",
    "print(len(set(group_indices)))\n",
    "group_map = dict(X_train[['group', 'split_group']].values.tolist())\n",
    "tta_group_map = dict(X_train[['tta_group', 'split_group']].values.tolist())\n",
    "group_map.update(tta_group_map)\n",
    "group_indices = [group_map[i] for i in group_indices]\n",
    "print(len(group_indices))\n",
    "print(len(set(group_indices)))\n",
    "folds = pd.read_csv('../input/ion-folds/folds.csv')\n",
    "folds_map = dict(folds[['split_group', 'fold']].values.tolist())\n",
    "group_indices = [folds_map[i] for i in group_indices]\n",
    "skf = GroupKFold(n_splits=CFG.n_fold)\n",
    "splits = [x for x in skf.split(sample_indices, None, group_indices)]\n",
    "predictions, groundtruth = [], []\n",
    "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
    "    if fold in CFG.fold:\n",
    "        with timer(f'##### Running Fold: {fold} #####'):\n",
    "            if fold==1:\n",
    "                model_path = MODEL_DIR1 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'\\\n",
    "                                + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\n",
    "            else:\n",
    "                model_path = MODEL_DIR2 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'\\\n",
    "                                + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\n",
    "            _predictions, _groundtruth = check_CV(fold, trn_idx, val_idx, model_path)\n",
    "            predictions.append(_predictions)\n",
    "            groundtruth.append(_groundtruth)\n",
    "predictions = np.concatenate(predictions)\n",
    "groundtruth = np.concatenate(groundtruth)\n",
    "score = f1_score(predictions, groundtruth, labels=list(range(11)), average='macro')\n",
    "logger.info(f'##### CV Score: {score} #####')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_probs = np.zeros((len(X_test), 11))\n",
    "total_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Get Sample function\n",
    "# =====================================================================================\n",
    "def get_sample_indices(df):\n",
    "    sample_indices = []\n",
    "    group_indices = []\n",
    "    df_groups = df.groupby('group').groups\n",
    "    for group_idx, indices in enumerate(df_groups.values()):\n",
    "        sample_indices.append(indices.values)\n",
    "        group_indices.append(group_idx)\n",
    "    return sample_indices, group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Inference\n",
    "# =====================================================================================\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def predict(model_path, X_test, test_samples):\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # Settings\n",
    "    # =====================================================================================\n",
    "    cate_cols = [c for c in X_train.columns if c.find('cate')>=0]\n",
    "    if fold==1:\n",
    "        cont_cols = ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
    "    else:\n",
    "        cont_cols = ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
    "    logger.info(f'cont_cols: {cont_cols}')\n",
    "    logger.info(f'cate_cols: {cate_cols}')\n",
    "    CFG.cont_cols = cont_cols\n",
    "    CFG.cate_cols = cate_cols\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f'device: {device}')\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare loader\n",
    "    # =====================================================================================\n",
    "    test_db = TestDataset(X_test, test_samples, CFG)\n",
    "    test_loader = DataLoader(test_db, batch_size=CFG.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare model\n",
    "    # =====================================================================================\n",
    "    model = Wavenet(CFG)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    logger.info(checkpoint['log'].sort_values(['VALID_SCORE']).tail(1))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # Inference\n",
    "    # =====================================================================================\n",
    "    prob = inference(test_loader, model, device)\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.18it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test['group'] = X_test.index // CFG.seq_len\n",
    "test_samples, _ = get_sample_indices(X_test)\n",
    "\n",
    "probs = []\n",
    "for fold in CFG.fold:\n",
    "    if fold==1:\n",
    "        model_path = MODEL_DIR1 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'\\\n",
    "                                + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\n",
    "    else:\n",
    "        model_path = MODEL_DIR2 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'\\\n",
    "                                + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\n",
    "    prob = predict(model_path, X_test, test_samples)\n",
    "    probs.append(prob)\n",
    "probs = np.mean(probs, axis=0)\n",
    "total_probs += probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.21it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.22it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.17it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.23it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.16it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.19it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.21it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.16it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0    87  0.001308    0.041772     0.940986    0.040835     0.942253\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.21it/s]\n",
      "cont_cols: ['signal', 'signal_lgb_oof', 'signal_cat_oof', 'signal_diff_max', 'signal_diff_min', 'signal_gradient']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cuda\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   121  0.000307    0.040285     0.942068    0.041028     0.942518\n",
      "100%|██████████| 13/13 [00:10<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for w in [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]:\n",
    "    \n",
    "    print(f'w: {w}')\n",
    "    tmp = pd.DataFrame()\n",
    "    for i in range(int(len(X_test)/500000)):\n",
    "        _tmp = X_test[i*500000+w:(i+1)*500000-(CFG.seq_len-w)]\n",
    "        tmp = pd.concat([tmp, _tmp])\n",
    "    idxes = tmp.index.tolist()\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    print(tmp.shape)\n",
    "\n",
    "    tmp['group'] = tmp.index // CFG.seq_len\n",
    "    test_samples, _ = get_sample_indices(tmp)\n",
    "\n",
    "    probs = []\n",
    "    for fold in CFG.fold:\n",
    "        if fold==1:\n",
    "            model_path = MODEL_DIR1 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'\\\n",
    "                                    + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\n",
    "        else:\n",
    "            model_path = MODEL_DIR2 + f'f-{fold}_b-{CFG.batch_size}_a-{CFG.encoder}_e-{CFG.emb_size}_h-{CFG.hidden_size}_'\\\n",
    "                                    + f'd-{CFG.dropout}_l-{CFG.nlayers}_hd-{CFG.nheads}_s-{CFG.seed}_len-{CFG.seq_len}.pt'\n",
    "        prob = predict(model_path, tmp, test_samples)\n",
    "        probs.append(prob)\n",
    "    tta_probs = np.zeros((len(X_test), 11))\n",
    "    tta_probs[idxes] = np.mean(probs, axis=0)\n",
    "    total_probs += tta_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = total_probs.argmax(1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4ea311efd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub = submission.copy()\n",
    "sub[TARGET] = predictions\n",
    "sub[TARGET] = sub[TARGET].astype(int)\n",
    "sub[TARGET].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
