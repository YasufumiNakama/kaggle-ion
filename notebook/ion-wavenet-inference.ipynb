{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Directory Settings\n",
    "# =====================================================================================\n",
    "ROOT = '../input/liverpool-ion-switching/'\n",
    "CLEAN_ROOT = '../input/data-without-drift/'\n",
    "OUTPUT_DIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    learning_rate=1.0e-3\n",
    "    batch_size=16\n",
    "    num_workers=6\n",
    "    print_freq=1000\n",
    "    test_freq=1\n",
    "    start_epoch=0\n",
    "    num_train_epochs=150\n",
    "    warmup_steps=30\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    weight_decay=0.01\n",
    "    dropout=0.3\n",
    "    emb_size=100\n",
    "    hidden_size=128\n",
    "    nlayers=2\n",
    "    nheads=10\n",
    "    seq_len=4000\n",
    "    total_cate_size=40\n",
    "    seed=1225\n",
    "    encoder='Wavenet'\n",
    "    optimizer='Adam' #@param ['AdamW', 'Adam']\n",
    "    target_size=11\n",
    "    n_fold=5\n",
    "    fold=[0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Library\n",
    "# =====================================================================================\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Utils\n",
    "# =====================================================================================\n",
    "def get_logger(filename=OUTPUT_DIR+'log'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    logger.info(f'[{name}] start')\n",
    "    yield\n",
    "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def load_df(path, debug=False):\n",
    "    # load df .csv or .pkl\n",
    "    if path.split('.')[-1]=='csv':\n",
    "        df = pd.read_csv(path)\n",
    "        if debug:\n",
    "            df = pd.read_csv(path, nrows=1000)\n",
    "    elif path.split('.')[-1]=='pkl':\n",
    "        df = pd.read_pickle(path)\n",
    "    print(f\"{path} shape / {df.shape} \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# General Settings\n",
    "# =====================================================================================\n",
    "df_path_dict = {'train': CLEAN_ROOT+'train_clean.csv',\n",
    "                'test': CLEAN_ROOT+'test_clean.csv',\n",
    "                'sample_submission': ROOT+'sample_submission.csv'}\n",
    "ID = 'time'\n",
    "TARGET = 'open_channels'\n",
    "SEED = 42\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/data-without-drift/train_clean.csv shape / (5000000, 3) \n",
      "../input/data-without-drift/test_clean.csv shape / (2000000, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] done in 4 s\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# Data Loading\n",
    "# =====================================================================================\n",
    "with timer('Data Loading'):\n",
    "    X_train = load_df(path=df_path_dict['train'])\n",
    "    X_test = load_df(path=df_path_dict['test'])\n",
    "    submission = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Preprocess\n",
    "# =====================================================================================\n",
    "def signal2cate(X_train, X_test=None, NUM_BINS=1000):\n",
    "    signal_bins = np.linspace(X_train['signal'].min(), X_train['signal'].max(), NUM_BINS + 1)\n",
    "    train_signal_dig = np.digitize(X_train['signal'], bins=signal_bins) - 1\n",
    "    train_signal_dig = np.minimum(train_signal_dig, len(signal_bins) - 2)\n",
    "    X_train['signal_cate'] = train_signal_dig\n",
    "    if X_test is not None:\n",
    "        test_signal_dig = np.digitize(X_test['signal'], bins=signal_bins) - 1\n",
    "        test_signal_dig = np.minimum(test_signal_dig, len(signal_bins) - 2)\n",
    "        X_test['signal_cate'] = test_signal_dig\n",
    "        return  X_train, X_test\n",
    "    return X_train\n",
    "\n",
    "X_train, X_test = signal2cate(X_train, X_test, NUM_BINS=CFG.total_cate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Dataset\n",
    "# =====================================================================================\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, sample_indices, cfg):\n",
    "        self.df = df.copy()\n",
    "        self.target = df[TARGET].values\n",
    "        self.sample_indices = sample_indices\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = self.cfg.seq_len\n",
    "        self.cont_cols = self.cfg.cont_cols\n",
    "        self.cate_cols = self.cfg.cate_cols\n",
    "        self.cont_df = self.df[self.cont_cols]\n",
    "        self.cate_df = self.df[self.cate_cols]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sample_indices[idx]\n",
    "        seq_len = min(self.seq_len, len(indices))\n",
    "\n",
    "        tmp_cont_x = torch.FloatTensor(self.cont_df.iloc[indices].values)\n",
    "        cont_x = torch.FloatTensor(self.seq_len, len(self.cont_cols)).zero_()\n",
    "        cont_x[-seq_len:] = tmp_cont_x[-seq_len:]\n",
    "\n",
    "        tmp_cate_x = torch.LongTensor(self.cate_df.iloc[indices].values)\n",
    "        cate_x = torch.LongTensor(self.seq_len, len(self.cate_cols)).zero_()\n",
    "        cate_x[-seq_len:] = tmp_cate_x[-seq_len:]\n",
    "\n",
    "        target = np.zeros(self.seq_len) - 1\n",
    "        target[-seq_len:] = self.target[indices]\n",
    "\n",
    "        return cate_x, cont_x, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, sample_indices, cfg):\n",
    "        self.df = df.copy()\n",
    "        self.sample_indices = sample_indices\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = self.cfg.seq_len\n",
    "        self.cont_cols = self.cfg.cont_cols\n",
    "        self.cate_cols = self.cfg.cate_cols\n",
    "        self.cont_df = self.df[self.cont_cols]\n",
    "        self.cate_df = self.df[self.cate_cols]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sample_indices[idx]\n",
    "        seq_len = min(self.seq_len, len(indices))\n",
    "\n",
    "        tmp_cont_x = torch.FloatTensor(self.cont_df.iloc[indices].values)\n",
    "        cont_x = torch.FloatTensor(self.seq_len, len(self.cont_cols)).zero_()\n",
    "        cont_x[-seq_len:] = tmp_cont_x[-seq_len:]\n",
    "\n",
    "        tmp_cate_x = torch.LongTensor(self.cate_df.iloc[indices].values)\n",
    "        cate_x = torch.LongTensor(self.seq_len, len(self.cate_cols)).zero_()\n",
    "        cate_x[-seq_len:] = tmp_cate_x[-seq_len:]\n",
    "\n",
    "        return cate_x, cont_x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Wavenet Model\n",
    "# =====================================================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class wave_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=int((kernel_size + (kernel_size-1)*(dilation-1))/2), dilation=dilation)\n",
    "        self.conv3 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=int((kernel_size + (kernel_size-1)*(dilation-1))/2), dilation=dilation)\n",
    "        self.conv4 = nn.Conv1d(out_ch, out_ch, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_x = x\n",
    "        tanh = self.tanh(self.conv2(x))\n",
    "        sig = self.sigmoid(self.conv3(x))\n",
    "        res = tanh.mul(sig)\n",
    "        x = self.conv4(res)\n",
    "        x = res_x + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wavenet(nn.Module):\n",
    "    def __init__(self, cfg, basic_block=wave_block):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.basic_block = basic_block\n",
    "        cont_col_size = len(cfg.cont_cols)\n",
    "        #cate_col_size = len(cfg.cate_cols)\n",
    "        self.cate_emb = nn.Embedding(cfg.total_cate_size, cfg.emb_size, padding_idx=0)\n",
    "        #self.cate_proj = nn.Sequential(\n",
    "        #    nn.Linear(cfg.emb_size*cate_col_size, cfg.hidden_size//2),\n",
    "        #    nn.LayerNorm(cfg.hidden_size//2),\n",
    "        #)\n",
    "        self.layer1 = self._make_layers(cont_col_size, cfg.hidden_size//16, 3, 12)\n",
    "        self.layer2 = self._make_layers(cfg.hidden_size//16, cfg.hidden_size//8, 3, 8)\n",
    "        self.layer3 = self._make_layers(cfg.hidden_size//8, cfg.hidden_size//4, 3, 4)\n",
    "        self.layer4 = self._make_layers(cfg.hidden_size//4, cfg.hidden_size//2, 3, 1)\n",
    "        self.gru = nn.GRU(input_size=cfg.emb_size, hidden_size=cfg.hidden_size//4, num_layers=cfg.nlayers,\n",
    "                          bidirectional=True, batch_first=True, dropout=cfg.dropout)\n",
    "        def get_reg():\n",
    "            return nn.Sequential(\n",
    "            nn.Linear(cfg.hidden_size, cfg.hidden_size),\n",
    "            nn.LayerNorm(cfg.hidden_size),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.hidden_size, cfg.hidden_size),\n",
    "            nn.LayerNorm(cfg.hidden_size),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.hidden_size, cfg.target_size),\n",
    "        )\n",
    "        self.fc = get_reg()\n",
    "\n",
    "    def _make_layers(self, in_ch, out_ch, kernel_size, n):\n",
    "        dilation_rates = [2 ** i for i in range(n)]\n",
    "        layers = [nn.Conv1d(in_ch, out_ch, 1)]\n",
    "        for dilation in dilation_rates:\n",
    "            layers.append(self.basic_block(out_ch, out_ch, kernel_size, dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, cate_x, cont_x):\n",
    "        batch_size = cate_x.size(0)\n",
    "        # RNN\n",
    "        cate_emb = self.cate_emb(cate_x).view(batch_size, self.cfg.seq_len, -1)\n",
    "        h_gru, _ = self.gru(cate_emb)\n",
    "        # CNN\n",
    "        cont_x = cont_x.permute(0, 2, 1)\n",
    "        cont_x = self.layer1(cont_x)\n",
    "        cont_x = self.layer2(cont_x)\n",
    "        cont_x = self.layer3(cont_x)\n",
    "        cont_x = self.layer4(cont_x)\n",
    "        # CNN & RNN\n",
    "        x = torch.cat((cont_x, h_gru.permute(0, 2, 1)), 1).permute(0, 2, 1)\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Get Sample function\n",
    "# =====================================================================================\n",
    "def get_sample_indices(df):\n",
    "    sample_indices = []\n",
    "    group_indices = []\n",
    "    df_groups = df.groupby('group').groups\n",
    "    for group_idx, indices in enumerate(df_groups.values()):\n",
    "        sample_indices.append(indices.values)\n",
    "        group_indices.append(group_idx)\n",
    "    return np.array(sample_indices), group_indices\n",
    "\n",
    "X_train['group'] = X_train.index // CFG.seq_len\n",
    "sample_indices, group_indices = get_sample_indices(X_train)\n",
    "\n",
    "X_test['group'] = X_test.index // CFG.seq_len\n",
    "test_samples, _ = get_sample_indices(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# validate function\n",
    "# =====================================================================================\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def validate(valid_loader, model, device):\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "    \n",
    "    tk0 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "\n",
    "    for step, (cate_x, cont_x, y) in tk0:\n",
    "\n",
    "        cate_x, cont_x, y = cate_x.to(device), cont_x.to(device), y.to(device)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            pred = model(cate_x, cont_x)\n",
    "\n",
    "            # record loss\n",
    "            pred_ = pred.view(-1, pred.shape[-1])\n",
    "            y_ = y.view(-1)\n",
    "\n",
    "        # record accuracy\n",
    "        val_true = torch.cat([val_true, y_.long()], 0)\n",
    "        val_preds = torch.cat([val_preds, pred_], 0)\n",
    "\n",
    "    # scoring\n",
    "    predictions = val_preds.cpu().detach().numpy().argmax(1)\n",
    "    groundtruth = val_true.cpu().detach().numpy()\n",
    "    score = f1_score(predictions, groundtruth, labels=list(range(11)), average='macro')\n",
    "    logger.info(f'score: {score}')\n",
    "\n",
    "    return predictions, groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Inference function\n",
    "# =====================================================================================\n",
    "def inference(test_loader, model, device):\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    probs = torch.Tensor([]).to(device)\n",
    "    \n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    \n",
    "    for step, (cate_x, cont_x) in tk0:      \n",
    "        \n",
    "        cate_x, cont_x = cate_x.to(device), cont_x.to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            pred = model(cate_x, cont_x)\n",
    "            pred_ = pred.view(-1, pred.shape[-1])\n",
    "        \n",
    "        probs = torch.cat([probs, pred_], 0)\n",
    "\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# CV\n",
    "# =====================================================================================\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def check_CV(fold, trn_idx, val_idx, model_path):\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # Settings\n",
    "    # =====================================================================================\n",
    "    cate_cols = ['signal_cate']\n",
    "    cont_cols = [c for c in X_train.columns if c.find('signal')>=0]\n",
    "    cont_cols = [c for c in cont_cols if c not in cate_cols]\n",
    "    logger.info(f'cont_cols: {cont_cols}')\n",
    "    logger.info(f'cate_cols: {cate_cols}')\n",
    "    CFG.cont_cols = cont_cols\n",
    "    CFG.cate_cols = cate_cols\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f'device: {device}')\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare loader\n",
    "    # =====================================================================================\n",
    "    #train_samples = sample_indices[trn_idx]\n",
    "    valid_samples = sample_indices[val_idx]\n",
    "\n",
    "    #train_db = TrainDataset(X_train, train_samples, CFG)\n",
    "    valid_db = TrainDataset(X_train, valid_samples, CFG)\n",
    "\n",
    "    #train_loader = DataLoader(train_db, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_db, batch_size=CFG.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare model\n",
    "    # =====================================================================================\n",
    "    model = Wavenet(CFG)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    logger.info(checkpoint['log'].sort_values(['VALID_SCORE']).tail(1))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # CV\n",
    "    # =====================================================================================\n",
    "    predictions, groundtruth = validate(valid_loader, model, device)\n",
    "    \n",
    "    return predictions, groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##### Running Fold: 0 #####] start\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   137  0.000019    0.044237     0.938147    0.044753        0.939\n",
      "100%|██████████| 16/16 [00:25<00:00,  1.59s/it]\n",
      "score: 0.9390004034771416\n",
      "[##### Running Fold: 0 #####] done in 26 s\n",
      "[##### Running Fold: 1 #####] start\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   128  0.000058    0.045614     0.937061    0.041508     0.940186\n",
      "100%|██████████| 16/16 [00:26<00:00,  1.66s/it]\n",
      "score: 0.9401858089363092\n",
      "[##### Running Fold: 1 #####] done in 27 s\n",
      "[##### Running Fold: 2 #####] start\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   143  0.000005    0.045537     0.936992    0.042839     0.938055\n",
      "100%|██████████| 16/16 [00:26<00:00,  1.63s/it]\n",
      "score: 0.9380554617816214\n",
      "[##### Running Fold: 2 #####] done in 27 s\n",
      "[##### Running Fold: 3 #####] start\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH       LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   106  0.00023    0.045589     0.936935     0.04414     0.937838\n",
      "100%|██████████| 16/16 [00:26<00:00,  1.66s/it]\n",
      "score: 0.9378378008373564\n",
      "[##### Running Fold: 3 #####] done in 27 s\n",
      "[##### Running Fold: 4 #####] start\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   146  0.000001    0.044391     0.938192    0.043412     0.938817\n",
      "100%|██████████| 16/16 [00:26<00:00,  1.64s/it]\n",
      "score: 0.9388172468788504\n",
      "[##### Running Fold: 4 #####] done in 27 s\n",
      "##### CV Score: 0.938820739395269 #####\n"
     ]
    }
   ],
   "source": [
    "skf = GroupKFold(n_splits=CFG.n_fold)\n",
    "splits = [x for x in skf.split(sample_indices, None, group_indices)]\n",
    "predictions, groundtruth = [], []\n",
    "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
    "    if fold in CFG.fold:\n",
    "        with timer(f'##### Running Fold: {fold} #####'):\n",
    "            model_path = f'../input/ion-wavenet-weight1/f-{fold}_b-16_a-Wavenet_e-100_h-128_d-0.3_l-2_hd-10_s-1225_len-4000.pt'\n",
    "            _predictions, _groundtruth = check_CV(fold, trn_idx, val_idx, model_path)\n",
    "            predictions.append(_predictions)\n",
    "            groundtruth.append(_groundtruth)\n",
    "predictions = np.concatenate(predictions)\n",
    "groundtruth = np.concatenate(groundtruth)\n",
    "score = f1_score(predictions, groundtruth, labels=list(range(11)), average='macro')\n",
    "logger.info(f'##### CV Score: {score} #####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Inference\n",
    "# =====================================================================================\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def predict(model_path):\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # Settings\n",
    "    # =====================================================================================\n",
    "    cate_cols = ['signal_cate']\n",
    "    cont_cols = [c for c in X_train.columns if c.find('signal')>=0]\n",
    "    cont_cols = [c for c in cont_cols if c not in cate_cols]\n",
    "    logger.info(f'cont_cols: {cont_cols}')\n",
    "    logger.info(f'cate_cols: {cate_cols}')\n",
    "    CFG.cont_cols = cont_cols\n",
    "    CFG.cate_cols = cate_cols\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f'device: {device}')\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare loader\n",
    "    # =====================================================================================\n",
    "    test_db = TestDataset(X_test, test_samples, CFG)\n",
    "    test_loader = DataLoader(test_db, batch_size=CFG.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # =====================================================================================\n",
    "    # Prepare model\n",
    "    # =====================================================================================\n",
    "    model = Wavenet(CFG)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    logger.info(checkpoint['log'].sort_values(['VALID_SCORE']).tail(1))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # Inference\n",
    "    # =====================================================================================\n",
    "    prob = inference(test_loader, model, device)\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   137  0.000019    0.044237     0.938147    0.044753        0.939\n",
      "100%|██████████| 32/32 [00:52<00:00,  1.64s/it]\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   128  0.000058    0.045614     0.937061    0.041508     0.940186\n",
      "100%|██████████| 32/32 [00:52<00:00,  1.65s/it]\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   143  0.000005    0.045537     0.936992    0.042839     0.938055\n",
      "100%|██████████| 32/32 [00:52<00:00,  1.64s/it]\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH       LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   106  0.00023    0.045589     0.936935     0.04414     0.937838\n",
      "100%|██████████| 32/32 [00:52<00:00,  1.65s/it]\n",
      "cont_cols: ['signal']\n",
      "cate_cols: ['signal_cate']\n",
      "device: cpu\n",
      "  EPOCH        LR  TRAIN_LOSS  TRAIN_SCORE  VALID_LOSS  VALID_SCORE\n",
      "0   146  0.000001    0.044391     0.938192    0.043412     0.938817\n",
      "100%|██████████| 32/32 [00:53<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "for fold in CFG.fold:\n",
    "    model_path = f'../input/ion-wavenet-weight1/f-{fold}_b-16_a-Wavenet_e-100_h-128_d-0.3_l-2_hd-10_s-1225_len-4000.pt'\n",
    "    prob = predict(model_path)\n",
    "    probs.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.mean(probs, axis=0).argmax(1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9142a72630>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub = submission.copy()\n",
    "sub[TARGET] = predictions\n",
    "sub[TARGET] = sub[TARGET].astype(int)\n",
    "sub[TARGET].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
